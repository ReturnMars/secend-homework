# 数据治理 Week 1：脏数据克星 —— 智能清洗工具 (ETL)

## 🎯 项目目标
数据治理的第一步是“把脏数据洗干净”。本周项目考察**正则表达式生成**、**文件流处理**以及 AI 辅助的**模糊逻辑判断**。
*   **核心痛点**：业务给的 Excel/CSV 总是格式混乱（日期格式不一、手机号带空格、地址混在一起）。
*   **AI 核心能力**：Regex 生成、异常数据修复逻辑。

## 📦 业务需求描述
开发一个 Web 小工具：
1.  **上传**：支持上传 CSV 或 Excel 文件（假设数据是用户花名册，包含姓名、手机、地址、入职日期）。
2.  **规则配置（清洗逻辑）**：
    *   **手机号标准化**：去除空格、横杠，非 11 位标记为“异常”。
    *   **日期统一**：将 `2023/1/1`, `2023-01-01`, `23年1月1日` 统一清洗为 `YYYY-MM-DD` 格式。
    *   **地址结构化（难点）**：从一串长文本（如“北京市朝阳区朝阳北路101号”）中自动提取“省”、“市”、“区”。
3.  **导出**：下载清洗后的干净文件，并将无法清洗的行单独导出为“异常报告”。

## 🤖 AI 提效挑战（必做）

### 1. 核心算法（正则与逻辑）
*   **挑战**：绝对不要手写正则表达式！
*   **Prompt 示例**：
    > "请帮我写一个正则表达式，用于匹配中国大陆手机号。要求：兼容带空格（138 0000 0000）、带横杠（138-0000-0000）和无前缀的情况。提取后只保留纯数字。"
*   **挑战**：地址提取（NLP 初探）。
    > "我有一列地址数据（例如：'浙江省杭州市西湖区...'）。请编写一个函数，利用正则或简单的逻辑，将地址拆分为 Province, City, District 三个字段。请考虑直辖市的情况（如上海市黄浦区）。"

### 2. 逻辑处理（Pandas/Stream）
*   **挑战**：利用 AI 快速处理数据帧或流。
*   **Python 组 Prompt**：
    > "使用 Pandas 读取 Excel。逻辑：如果 '入职日期' 列解析失败，尝试用多种格式（YYYY/MM/DD, DD-MM-YYYY）去匹配，如果都失败则填入 NaT。"
*   **Java/Node 组 Prompt**：
    > "读取 CSV 文件流。请帮我写一段代码，当遇到空行或列数不对的行时，自动跳过并记录行号到错误日志中，不要中断程序。"

## 🛠 技术要求
*   **技术栈**：不限（Python Pandas 最快，但 Java/Node 也可以通过 stream 库实现）。
*   **交互**：简单的文件上传框和下载按钮即可。

## 📅 交付物
1.  **源码仓库**。
2.  **测试文件**：你需要提供一个包含各种奇葩脏数据的 CSV（作为测试用例），演示你的工具能把它洗干净。