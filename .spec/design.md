# åç«¯æŠ€æœ¯è®¾è®¡

## ä¸€ã€æ¶æ„æ¦‚è§ˆ

### 1.1 ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              æµè§ˆå™¨                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Next.js å‰ç«¯ (å·²å®Œæˆ)                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   Chat Panel    â”‚   Artifact   â”‚    Sandpack Preview     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ (Vercel AI SDK) â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    (React æ²™ç®±æ‰§è¡Œ)     â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚ SSE Stream                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Python åç«¯æœåŠ¡ (æœ¬è®¾è®¡)                             â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                           FastAPI                                   â”‚ â”‚
â”‚  â”‚                                                                     â”‚ â”‚
â”‚  â”‚   /api/chat â”€â”€â”€â”€â”€â”€â”€â”€â–¶ ChatService â”€â”€â”€â”€â”€â”€â”€â”€â–¶ LLMClient              â”‚ â”‚
â”‚  â”‚       â”‚                    â”‚                    â”‚                   â”‚ â”‚
â”‚  â”‚       â”‚                    â–¼                    â”‚                   â”‚ â”‚
â”‚  â”‚       â”‚              MemoryManager              â”‚                   â”‚ â”‚
â”‚  â”‚       â”‚                    â”‚                    â”‚                   â”‚ â”‚
â”‚  â”‚       â–¼                    â–¼                    â–¼                   â”‚ â”‚
â”‚  â”‚  StreamHandler â—€â”€â”€â”€â”€ SystemPrompt â—€â”€â”€â”€â”€ LangChain                  â”‚ â”‚
â”‚  â”‚       â”‚                                                             â”‚ â”‚
â”‚  â”‚       â–¼                                                             â”‚ â”‚
â”‚  â”‚   AIStreamBuilder (fastapi-ai-sdk)                                  â”‚ â”‚
â”‚  â”‚       â”‚                                                             â”‚ â”‚
â”‚  â”‚       â–¼                                                             â”‚ â”‚
â”‚  â”‚   SSE Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ å‰ç«¯    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        å¤–éƒ¨ LLM æœåŠ¡                               â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚   â”‚   OpenAI    â”‚   â”‚  Anthropic  â”‚   â”‚   Google    â”‚            â”‚  â”‚
â”‚  â”‚   â”‚   GPT-4o    â”‚   â”‚   Claude    â”‚   â”‚   Gemini    â”‚            â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ç‰ˆæœ¬ | è¯´æ˜ |
|-----|------|------|------|
| Web æ¡†æ¶ | FastAPI | 0.115+ | å¼‚æ­¥ Web æ¡†æ¶ |
| LLM æ¡†æ¶ | LangChain | 0.3+ | å¤šæ¨¡å‹é›†æˆ |
| AI SDK å…¼å®¹ | fastapi-ai-sdk | 0.2+ | Vercel AI SDK åè®® |
| æ•°æ®éªŒè¯ | Pydantic | 2.0+ | ç±»å‹å®‰å…¨ |
| æœåŠ¡å™¨ | Uvicorn | 0.30+ | ASGI æœåŠ¡å™¨ |

---

## äºŒã€é¡¹ç›®ç»“æ„

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # åº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # API è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ router.py           # è·¯ç”±æ³¨å†Œ
â”‚   â”‚   â”œâ”€â”€ chat.py             # POST /api/chat
â”‚   â”‚   â”œâ”€â”€ sessions.py         # ä¼šè¯ç®¡ç†æ¥å£
â”‚   â”‚   â””â”€â”€ settings.py         # è®¾ç½®æ¥å£
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒé€»è¾‘
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ client.py       # LLM å®¢æˆ·ç«¯
â”‚   â”‚   â”‚   â””â”€â”€ providers.py    # æä¾›å•†é…ç½®
â”‚   â”‚   â”œâ”€â”€ stream_parser.py    # æµå¼è§£æå™¨
â”‚   â”‚   â””â”€â”€ memory.py           # ä¼šè¯å†å²
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/                # Prompt æ¨¡æ¿ä¸çº¦æŸ
â”‚   â”‚   â”œâ”€â”€ __init__.py         # ç»Ÿä¸€å¯¼å‡ºå…¥å£
â”‚   â”‚   â”œâ”€â”€ shared.py           # ğŸ”‘ å…¬å…±çº¦æŸä¸­å¿ƒ (SYNTAX_RULES, NAMING_RULES, etc.)
â”‚   â”‚   â”œâ”€â”€ system.py           # ç›´æ¥å¯¹è¯æ¨¡å¼ System Prompt
â”‚   â”‚   â”œâ”€â”€ planner.py          # Planner AI æç¤ºè¯
â”‚   â”‚   â”œâ”€â”€ worker.py           # Worker AI æç¤ºè¯
â”‚   â”‚   â””â”€â”€ data_aware.py       # æ•°æ®ä¸Šä¸‹æ–‡æ„å»ºå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # Pydantic æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ session.py
â”‚   â”‚   â””â”€â”€ errors.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_chat.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ Dockerfile
```

---

## ä¸‰ã€æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 3.1 é…ç½®æ¨¡å— (config.py)

```python
from pydantic_settings import BaseSettings
from typing import Literal
from functools import lru_cache


class Settings(BaseSettings):
    """åº”ç”¨é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–"""
    
    # åº”ç”¨
    app_name: str = "Nexus AI Backend"
    debug: bool = False
    cors_origins: list[str] = ["http://localhost:3000"]
    
    # LLM é»˜è®¤é…ç½®
    default_provider: Literal["openai", "anthropic", "google"] = "openai"
    default_model: str = "gpt-4o"
    llm_temperature: float = 0.7
    llm_max_tokens: int = 4096
    
    # API Keys (å¯é€‰ï¼Œä¹Ÿå¯ç”±å‰ç«¯ä¼ å…¥)
    openai_api_key: str | None = None
    anthropic_api_key: str | None = None
    google_api_key: str | None = None
    
    # ä¼šè¯é…ç½®
    session_ttl_seconds: int = 3600
    max_history_messages: int = 50
    
    class Config:
        env_file = ".env"


@lru_cache
def get_settings() -> Settings:
    return Settings()
```

### 3.2 LLM å®¢æˆ·ç«¯ (core/llm/client.py)

```python
from langchain.chat_models import init_chat_model
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, SystemMessage
from typing import AsyncIterator, Literal

from app.config import get_settings
from app.prompts.system import SYSTEM_PROMPT


Provider = Literal["openai", "anthropic", "google"]


class LLMClient:
    """
    å¤šæ¨¡å‹ LLM å®¢æˆ·ç«¯
    ä½¿ç”¨ LangChain init_chat_model ç»Ÿä¸€æ¥å£
    
    æ”¯æŒæ ¼å¼:
    - init_chat_model("openai:gpt-4o")
    - init_chat_model("anthropic:claude-sonnet-4-5-20250514")
    """
    
    # å„æä¾›å•†é»˜è®¤æ¨¡å‹
    DEFAULT_MODELS = {
        "openai": "gpt-4o",
        "anthropic": "claude-sonnet-4-5-20250514",
        "google": "gemini-2.0-flash",
    }
    
    def __init__(
        self,
        provider: Provider = "openai",
        model: str | None = None,
        api_key: str | None = None,
    ):
        self.provider = provider
        self.model = model or self.DEFAULT_MODELS.get(provider, "gpt-4o")
        self.api_key = api_key
        self._client: BaseChatModel | None = None
    
    @property
    def client(self) -> BaseChatModel:
        """æ‡’åŠ è½½ LangChain å®¢æˆ·ç«¯"""
        if self._client is None:
            settings = get_settings()
            
            # API Key ä¼˜å…ˆçº§: å‚æ•° > ç¯å¢ƒå˜é‡
            api_key = self.api_key or getattr(
                settings, 
                f"{self.provider}_api_key", 
                None
            )
            
            if not api_key:
                raise ValueError(f"API key required for {self.provider}")
            
            # ä½¿ç”¨ "provider:model" æ ¼å¼åˆå§‹åŒ–
            model_string = f"{self.provider}:{self.model}"
            
            self._client = init_chat_model(
                model_string,
                api_key=api_key,
                temperature=settings.llm_temperature,
                max_tokens=settings.llm_max_tokens,
            )
        return self._client
    
    async def astream(
        self,
        messages: list[BaseMessage],
        system_prompt: str | None = None,
    ) -> AsyncIterator[str]:
        """
        å¼‚æ­¥æµå¼ç”Ÿæˆå“åº”
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            system_prompt: å¯é€‰çš„ç³»ç»Ÿæç¤ºè¯
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        full_messages = [
            SystemMessage(content=system_prompt or SYSTEM_PROMPT)
        ] + messages
        
        # å¼‚æ­¥æµå¼è¾“å‡º
        async for chunk in self.client.astream(full_messages):
            # chunk æ˜¯ AIMessageChunkï¼Œcontent å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
            if chunk.content:
                if isinstance(chunk.content, str):
                    yield chunk.content
                elif isinstance(chunk.content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹å—
                    for block in chunk.content:
                        if isinstance(block, dict) and block.get("type") == "text":
                            yield block.get("text", "")
                        elif isinstance(block, str):
                            yield block
```

### 3.3 æµå¼è§£æå™¨ (core/stream_parser.py)

```python
from enum import Enum
from dataclasses import dataclass

class EventType(str, Enum):
    THINKING = "thinking"
    MESSAGE = "message"
    ARTIFACT_START = "artifact_start"
    ARTIFACT_CODE = "artifact_code"
    ARTIFACT_END = "artifact_end"
    SESSION_ID = "session_id"
    ERROR = "error"

class StreamParser:
    """
    è´Ÿè´£å°† LLM çš„æ–‡æœ¬æµè§£æä¸ºç»“æ„åŒ–çš„ SSE äº‹ä»¶ã€‚
    è¯†åˆ« <think> å’Œ <artifact> æ ‡ç­¾ï¼Œå¹¶è½¬æ¢ä¸ºå¯¹åº”çš„äº‹ä»¶æµã€‚
    """
    def feed(self, chunk: str) -> list[StreamEvent]:
        # å†…éƒ¨çŠ¶æ€æœºé€»è¾‘ï¼Œå¤„ç†æ ‡ç­¾è¾¹ç•Œå’Œå†…å®¹ç¼“å†²
        pass
```

### 3.4 Chat Service (api/chat.py)

```python
async def generate_stream(llm_client, messages, session_id):
    # 1. å‘é€ Session ID
    if session_id:
        yield StreamEvent(type=EventType.SESSION_ID, content=session_id).to_sse()

    # 2. è°ƒç”¨ LLM å¹¶é€šè¿‡ StreamParser è§£æ
    async for chunk in llm_client.astream(messages):
        for event in parser.feed(chunk):
            yield event.to_sse()
```

### 3.4 ä¼šè¯å†å²ç®¡ç† (core/memory.py)

```python
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from typing import Dict, List
from datetime import datetime
import asyncio

from app.config import get_settings


class InMemoryStore:
    """å†…å­˜ä¼šè¯å­˜å‚¨"""
    
    def __init__(self):
        self._sessions: Dict[str, Dict] = {}
        self._lock = asyncio.Lock()
    
    async def get_messages(self, session_id: str) -> List[BaseMessage]:
        async with self._lock:
            session = self._sessions.get(session_id, {})
            return session.get("messages", [])
    
    async def add_message(self, session_id: str, message: BaseMessage):
        settings = get_settings()
        
        async with self._lock:
            if session_id not in self._sessions:
                self._sessions[session_id] = {
                    "messages": [],
                    "created_at": datetime.now(),
                    "updated_at": datetime.now(),
                }
            
            session = self._sessions[session_id]
            session["messages"].append(message)
            session["updated_at"] = datetime.now()
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(session["messages"]) > settings.max_history_messages:
                session["messages"] = session["messages"][-settings.max_history_messages:]
    
    async def delete_session(self, session_id: str) -> bool:
        async with self._lock:
            if session_id in self._sessions:
                del self._sessions[session_id]
                return True
            return False
    
    async def list_sessions(self) -> List[dict]:
        async with self._lock:
            return [
                {
                    "session_id": sid,
                    "message_count": len(s["messages"]),
                    "created_at": s["created_at"].isoformat(),
                    "updated_at": s["updated_at"].isoformat(),
                }
                for sid, s in self._sessions.items()
            ]
    
    async def get_session(self, session_id: str) -> dict | None:
        async with self._lock:
            session = self._sessions.get(session_id)
            if not session:
                return None
            return {
                "session_id": session_id,
                "message_count": len(session["messages"]),
                "messages": [
                    {
                        "role": "user" if isinstance(m, HumanMessage) else "assistant",
                        "content": m.content,
                    }
                    for m in session["messages"]
                ],
                "created_at": session["created_at"].isoformat(),
                "updated_at": session["updated_at"].isoformat(),
            }


# å•ä¾‹
_memory_instance: InMemoryStore | None = None

def get_memory() -> InMemoryStore:
    global _memory_instance
    if _memory_instance is None:
        _memory_instance = InMemoryStore()
    return _memory_instance
```

### 3.5 Prompt æ¨¡å—æ¶æ„ (prompts/)

é¡¹ç›®ä½¿ç”¨åˆ†å±‚çš„ **Prompt çº¦æŸæ¶æ„**ï¼Œç¡®ä¿æ‰€æœ‰ AI ä»£ç†ç”Ÿæˆä¸€è‡´ã€æ— é”™è¯¯çš„ä»£ç ã€‚

#### 3.5.1 å…±äº«çº¦æŸ (prompts/shared.py)

```python
# Recharts ç»„ä»¶å¼ºåˆ¶åˆ«åæ˜ å°„ (é¿å…ä¸ lucide-react å†²çª)
RECHARTS_ALIAS_MAP = {
    "PieChart": "RePieChart",
    "BarChart": "ReBarChart",
    "LineChart": "ReLineChart",
    # ...
}

# å¸¸ç”¨ Lucide å›¾æ ‡
LUCIDE_COMMON_ICONS = ["TrendingUp", "TrendingDown", "Clock", "DollarSign", ...]

# ç¦ç”¨çš„ TypeScript è¯­æ³•æ¨¡å¼ (é˜²æ­¢ SyntaxError)
SYNTAX_RULES = """
| Pattern | Why It Crashes | Safe Alternative |
|---------|---------------|------------------|
| `x as Type` | TS type assertion | Use `const x: Type = ...` |
| `x as keyof typeof obj` | TS advanced typing | Use `obj[x]` |
| `: value is Type` | TS type predicate | Remove return type |
"""

# å‘½åè§„èŒƒ (é˜²æ­¢å…¨å±€ä½œç”¨åŸŸå†²çª)
NAMING_RULES = """
- æ‰€æœ‰é¡¶å±‚å®šä¹‰å¿…é¡»ä½¿ç”¨ `ComponentId_` å‰ç¼€
- å¸¸é‡å’Œè¾…åŠ©å‡½æ•°åº”å®šä¹‰åœ¨ç»„ä»¶å†…éƒ¨
"""

# Recharts ä½¿ç”¨è§„èŒƒ
RECHARTS_RULES = """
- å¿…é¡»ä½¿ç”¨ RePieChart, ReBarChart ç­‰åˆ«å
- å¿…é¡»ç”¨ ResponsiveContainer åŒ…è£¹
- Pie å†…éƒ¨å¿…é¡»æœ‰ Cell
"""

# è¾“å‡ºæ ¼å¼è§„èŒƒ
OUTPUT_RULES = """
- åªè¿”å›åŸå§‹ä»£ç ï¼Œä¸åŒ…å« Markdown ä»£ç å—
- ä¸åŒ…å« import è¯­å¥ï¼ˆç”± Assembler æ³¨å…¥ï¼‰
"""

# ä¾¿æ·ç»„åˆ
WORKER_CONSTRAINTS = get_shared_constraints(include_recharts=True, include_output_rules=True)
PLANNER_CONSTRAINTS = get_shared_constraints(include_recharts=False, include_output_rules=False)
```

#### 3.5.2 Planner æç¤ºè¯ (prompts/planner.py)

```python
# ç”¨äºç”Ÿæˆ Blueprint (ç»„ä»¶è§„åˆ’)
PLANNER_SYSTEM_PROMPT = """You are an expert Dashboard Architect...

## Key Design Principles
1. Component Atomicity - æ¯ä¸ªç»„ä»¶ä¸“æ³¨äºä¸€ä¸ªå¯è§†åŒ–
2. Data Access Authority - ç²¾ç¡®çš„æ•°æ®è®¿é—®è·¯å¾„
3. TypeScript Interface Generation - ç®€å•æ¥å£ï¼Œç¦æ­¢é«˜çº§ TS è¯­æ³•
"""
```

#### 3.5.3 Worker æç¤ºè¯ (prompts/worker.py)

```python
from app.prompts.shared import WORKER_CONSTRAINTS

WORKER_SYSTEM_PROMPT = f"""You are a React Component Specialist...

{WORKER_CONSTRAINTS}  # æ³¨å…¥å…±äº«çº¦æŸ

## Example Output
interface SalesChart_Props {{ ... }}
function SalesChart({{ data }}: SalesChart_Props) {{ ... }}
"""
```

#### 3.5.4 System Prompt (prompts/system.py) - ç›´æ¥å¯¹è¯æ¨¡å¼

```python
from app.prompts.shared import SYNTAX_RULES, RECHARTS_RULES

SYSTEM_PROMPT = f"""You are Nexus AI, an expert React UI Engineer...

{SYNTAX_RULES}   # æ³¨å…¥è¯­æ³•çº¦æŸ
{RECHARTS_RULES} # æ³¨å…¥ Recharts è§„èŒƒ

## Output Format
<artifact identifier="kebab-case-id" type="react" title="æ ‡é¢˜">
export default function App() {{ ... }}
</artifact>
"""
```

#### 3.5.5 ä½¿ç”¨æ–¹å¼

```python
# åœ¨ Worker ä¸­ä½¿ç”¨
from app.prompts.worker import WORKER_SYSTEM_PROMPT, build_worker_prompt

# åœ¨ Planner ä¸­ä½¿ç”¨
from app.prompts.planner import PLANNER_SYSTEM_PROMPT, build_planner_prompt

# åœ¨ Assembler ä¸­ä½¿ç”¨å…±äº«å¸¸é‡
from app.prompts.shared import RECHARTS_ALIAS_MAP, LUCIDE_COMMON_ICONS
```

---

## å››ã€API è·¯ç”±è®¾è®¡

### 4.1 è·¯ç”±æ³¨å†Œ (api/router.py)

```python
from fastapi import APIRouter

from app.api import chat, sessions, settings

api_router = APIRouter()

api_router.include_router(chat.router, tags=["chat"])
api_router.include_router(sessions.router, prefix="/sessions", tags=["sessions"])
api_router.include_router(settings.router, prefix="/settings", tags=["settings"])
```

### 4.2 èŠå¤©æ¥å£ (api/chat.py)

```python
from fastapi import APIRouter
from fastapi.responses import StreamingResponse

from app.models.chat import ChatRequest
from app.api.deps import LLMClientDep

router = APIRouter()


@router.post("/chat")
async def chat(request: ChatRequest, llm_client: LLMClientDep):
    """
    æµå¼èŠå¤©æ¥å£
    
    è¿”å› SSE æ ¼å¼ï¼ŒåŒ…å« thinking, message, artifact ç­‰è‡ªå®šä¹‰äº‹ä»¶
    """
    return StreamingResponse(
        generate_stream(
            llm_client=llm_client,
            messages=request.messages,
            session_id=request.session_id,
        ),
        media_type="text/event-stream"
    )
```

### 4.3 ä¼šè¯æ¥å£ (api/sessions.py)

```python
from fastapi import APIRouter, HTTPException

from app.core.memory import get_memory
from app.models.session import SessionResponse, SessionListResponse

router = APIRouter()


@router.get("", response_model=SessionListResponse)
async def list_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    memory = get_memory()
    sessions = await memory.list_sessions()
    return {"sessions": sessions}


@router.get("/{session_id}", response_model=SessionResponse)
async def get_session(session_id: str):
    """è·å–ä¼šè¯è¯¦æƒ…"""
    memory = get_memory()
    session = await memory.get_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    return session


@router.delete("/{session_id}")
async def delete_session(session_id: str):
    """åˆ é™¤ä¼šè¯"""
    memory = get_memory()
    deleted = await memory.delete_session(session_id)
    if not deleted:
        raise HTTPException(status_code=404, detail="Session not found")
    return {"success": True}
```

### 4.4 è®¾ç½®æ¥å£ (api/settings.py)

```python
from fastapi import APIRouter

from app.config import get_settings
from app.core.llm.providers import PROVIDER_MODELS
from app.models.settings import ModelsResponse, ValidateKeyRequest, ValidateKeyResponse

router = APIRouter()


@router.get("/models", response_model=ModelsResponse)
async def get_models():
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    settings = get_settings()
    
    providers = []
    for provider_id, models in PROVIDER_MODELS.items():
        api_key = getattr(settings, f"{provider_id}_api_key", None)
        providers.append({
            "id": provider_id,
            "name": provider_id.title(),
            "models": models,
            "configured": bool(api_key),
        })
    
    return {
        "providers": providers,
        "default_provider": settings.default_provider,
        "default_model": settings.default_model,
    }


@router.post("/validate-key", response_model=ValidateKeyResponse)
async def validate_key(request: ValidateKeyRequest):
    """éªŒè¯ API Key æœ‰æ•ˆæ€§"""
    from app.core.llm.client import LLMClient
    
    try:
        client = LLMClient(
            provider=request.provider,
            api_key=request.api_key,
        )
        # å‘é€æµ‹è¯•è¯·æ±‚
        async for _ in client.astream([]):
            break
        return {"valid": True}
    except Exception as e:
        return {"valid": False, "error": str(e)}
```

---

## äº”ã€Pydantic æ¨¡å‹

### 5.1 èŠå¤©æ¨¡å‹ (models/chat.py)

```python
from pydantic import BaseModel
from typing import Literal


class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str


class ChatRequest(BaseModel):
    messages: list[ChatMessage]
    session_id: str | None = None
    provider: str | None = None
    model: str | None = None
    api_key: str | None = None
```

### 5.2 ä¼šè¯æ¨¡å‹ (models/session.py)

```python
from pydantic import BaseModel


class SessionInfo(BaseModel):
    session_id: str
    message_count: int
    created_at: str
    updated_at: str


class SessionResponse(SessionInfo):
    messages: list[dict]


class SessionListResponse(BaseModel):
    sessions: list[SessionInfo]
```

### 5.3 è®¾ç½®æ¨¡å‹ (models/settings.py)

```python
from pydantic import BaseModel


class ModelInfo(BaseModel):
    id: str
    name: str
    description: str | None = None


class ProviderInfo(BaseModel):
    id: str
    name: str
    models: list[ModelInfo]
    configured: bool


class ModelsResponse(BaseModel):
    providers: list[ProviderInfo]
    default_provider: str
    default_model: str


class ValidateKeyRequest(BaseModel):
    provider: str
    api_key: str


class ValidateKeyResponse(BaseModel):
    valid: bool
    error: str | None = None
```

---

## å…­ã€åº”ç”¨å…¥å£ (main.py)

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.config import get_settings
from app.api.router import api_router


def create_app() -> FastAPI:
    settings = get_settings()
    
    app = FastAPI(
        title=settings.app_name,
        version="0.1.0",
    )
    
    # CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.cors_origins,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # è·¯ç”±
    app.include_router(api_router, prefix="/api")
    
    # å¥åº·æ£€æŸ¥
    @app.get("/health")
    async def health():
        return {"status": "healthy"}
    
    return app


app = create_app()
```

---

## ä¸ƒã€ä¾èµ–æ¸…å• (requirements.txt)

```

# Web
fastapi>=0.115.0
uvicorn[standard]>=0.30.0
python-multipart>=0.0.9

# LangChain
langchain>=0.3.0
langchain-core>=0.3.0
langchain-openai>=0.2.0
langchain-anthropic>=0.2.0
langchain-google-genai>=2.0.0

# é…ç½®
pydantic>=2.0.0
pydantic-settings>=2.0.0
python-dotenv>=1.0.0

# æ—¥å¿—
loguru>=0.7.0
```

